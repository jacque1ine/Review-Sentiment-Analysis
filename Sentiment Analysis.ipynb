{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "xFeI61HGY2Kt"
      ]
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "svLhCiU3Evm6"
      },
      "source": [
        "# Yelp Review Sentiment Classification\n",
        "\n",
        "A classifier that can predict a user's rating of a given restaurant from their review.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BZgELVS8I6kc"
      },
      "source": [
        "![Example of a Yelp review](https://wordstream-files-prod.s3.amazonaws.com/s3fs-public/styles/simple_image/public/images/yelp-reviews-filtered.png)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2jS5ThMCEvnC",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 443
        },
        "outputId": "323ea11a-99d9-4674-cd26-9de743d7b391"
      },
      "source": [
        "#@title Import our libraries { display-mode: \"both\" }\n",
        "import pandas as pd   # Great for tables (google spreadsheets, microsoft excel, csv). \n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import string\n",
        "import nltk\n",
        "import spacy\n",
        "import wordcloud\n",
        "import os # Good for navigating your computer's files \n",
        "import sys\n",
        "\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize\n",
        "from spacy.lang.en.stop_words import STOP_WORDS\n",
        "nltk.download('wordnet')\n",
        "nltk.download('punkt')\n",
        "\n",
        "from wordcloud import WordCloud\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import confusion_matrix, classification_report\n",
        "!python -m spacy download en_core_web_md\n",
        "import en_core_web_md\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "NotImplementedError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNotImplementedError\u001b[0m                       Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-1-23daae3dec90>\u001b[0m in \u001b[0;36m<cell line: 27>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinear_model\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mLogisticRegression\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmetrics\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mconfusion_matrix\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mclassification_report\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 27\u001b[0;31m \u001b[0mget_ipython\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msystem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'python -m spacy download en_core_web_md'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     28\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0men_core_web_md\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/google/colab/_shell.py\u001b[0m in \u001b[0;36msystem\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m     97\u001b[0m       \u001b[0mkwargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0;34m'also_return_output'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     98\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 99\u001b[0;31m     \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_system_commands\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_system_compat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pylint:disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    100\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    101\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mpip_warn\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/google/colab/_system_commands.py\u001b[0m in \u001b[0;36m_system_compat\u001b[0;34m(shell, cmd, also_return_output)\u001b[0m\n\u001b[1;32m    451\u001b[0m   \u001b[0;31m# is expected to call this function, thus adding one level of nesting to the\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    452\u001b[0m   \u001b[0;31m# stack.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 453\u001b[0;31m   result = _run_command(\n\u001b[0m\u001b[1;32m    454\u001b[0m       \u001b[0mshell\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvar_expand\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcmd\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdepth\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mclear_streamed_output\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    455\u001b[0m   )\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/google/colab/_system_commands.py\u001b[0m in \u001b[0;36m_run_command\u001b[0;34m(cmd, clear_streamed_output)\u001b[0m\n\u001b[1;32m    165\u001b[0m   \u001b[0mlocale_encoding\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlocale\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgetpreferredencoding\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    166\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0mlocale_encoding\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0m_ENCODING\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 167\u001b[0;31m     raise NotImplementedError(\n\u001b[0m\u001b[1;32m    168\u001b[0m         \u001b[0;34m'A UTF-8 locale is required. Got {}'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlocale_encoding\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    169\u001b[0m     )\n",
            "\u001b[0;31mNotImplementedError\u001b[0m: A UTF-8 locale is required. Got ANSI_X3.4-1968"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KsF8tioZLicU"
      },
      "source": [
        "#@title Import our data\n",
        "\n",
        "import gdown\n",
        "gdown.download('https://drive.google.com/uc?id=1u0tnEF2Q1a7H_gUEH-ZB3ATx02w8dF4p', 'yelp_final.csv', True)\n",
        "data_file  = 'yelp_final.csv'\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DQ267zCBOjet"
      },
      "source": [
        "## Data Exploration"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_BLs_2JkEvnw"
      },
      "source": [
        "Look at the data available."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8dZ_lymcN_K9"
      },
      "source": [
        "yelp = pd.read_csv(data_file)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hp0c1vAdEvoF",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 203
        },
        "outputId": "b936afee-ede1-47e9-e29b-3e1539a236d4"
      },
      "source": [
        "#@title Show data\n",
        "yelp.head()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>business_id</th>\n",
              "      <th>stars</th>\n",
              "      <th>text</th>\n",
              "      <th>user_id</th>\n",
              "      <th>cool</th>\n",
              "      <th>useful</th>\n",
              "      <th>funny</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>9yKzy9PApeiPPOUJEtnvkg</td>\n",
              "      <td>5</td>\n",
              "      <td>My wife took me here on my birthday for breakf...</td>\n",
              "      <td>rLtl8ZkDX5vH5nAx9C3q5Q</td>\n",
              "      <td>2</td>\n",
              "      <td>5</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>ZRJwVLyzEJq1VAihDhYiow</td>\n",
              "      <td>5</td>\n",
              "      <td>I have no idea why some people give bad review...</td>\n",
              "      <td>0a2KyEL0d3Yb1V6aivbIuQ</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>_1QQZuf4zZOyFCvXc0o6Vg</td>\n",
              "      <td>5</td>\n",
              "      <td>Rosie, Dakota, and I LOVE Chaparral Dog Park!!...</td>\n",
              "      <td>uZetl9T0NcROGOyFfughhg</td>\n",
              "      <td>1</td>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>6ozycU1RpktNG2-1BroVtw</td>\n",
              "      <td>5</td>\n",
              "      <td>General Manager Scott Petello is a good egg!!!...</td>\n",
              "      <td>vYmM4KTsC8ZfQBg-j5MWkw</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>zp713qNhx8d9KCJJnrw1xA</td>\n",
              "      <td>5</td>\n",
              "      <td>Drop what you're doing and drive here. After I...</td>\n",
              "      <td>wFweIWhv2fREZV_dYkz_1g</td>\n",
              "      <td>7</td>\n",
              "      <td>7</td>\n",
              "      <td>4</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "              business_id  stars  ... useful funny\n",
              "0  9yKzy9PApeiPPOUJEtnvkg      5  ...      5     0\n",
              "1  ZRJwVLyzEJq1VAihDhYiow      5  ...      0     0\n",
              "2  _1QQZuf4zZOyFCvXc0o6Vg      5  ...      2     0\n",
              "3  6ozycU1RpktNG2-1BroVtw      5  ...      0     0\n",
              "4  zp713qNhx8d9KCJJnrw1xA      5  ...      7     4\n",
              "\n",
              "[5 rows x 7 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QjL5FrSLEvoP"
      },
      "source": [
        "Have access to 7 columns of data but business_id and user_id information are not important.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pB1yKcUtcpg9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 187
        },
        "outputId": "a0a17cde-75eb-4207-97c9-237d3ffc225a"
      },
      "source": [
        "#@title **Remove unnecessary columns** { display-mode: \"both\" }\n",
        "yelp.drop(labels=['business_id','user_id'],inplace=True,axis=1)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-2-8a972ec26849>\u001b[0m in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m#@title **Run to remove unnecessary columns** { display-mode: \"form\" }\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0myelp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdrop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlabels\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'business_id'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'user_id'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0minplace\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m: name 'yelp' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "la3rUPKgEvoi",
        "cellView": "form",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0755f36e-87b6-4cbd-9723-1f3c9abd42fd"
      },
      "source": [
        "#@title Check the text in differently rated reviews\n",
        "num_stars =  1#@param {type:\"integer\"}\n",
        "\n",
        "for t in yelp[yelp['stars'] == num_stars]['text'].head(20).values:\n",
        "    print (t) "
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "U can go there n check the car out. If u wanna buy 1 there? That's wrong move! If u even want a car service from there? U made a biggest mistake of ur life!! I had 1 time asked my girlfriend to take my car there for an oil service, guess what? They ripped my girlfriend off by lying how bad my car is now. If without fixing the problem. Might bring some serious accident. Then she did what they said. 4 brand new tires, timing belt, 4 new brake pads. U know why's the worst? All of those above I had just changed 2 months before!!! What a trashy dealer is that? People, better off go somewhere!\n",
            "Disgusting!  Had a Groupon so my daughter and I tried it out.  Very outdated and gaudy 80's style interior made me feel like I was in an episode of Sopranos.  The food itself was pretty bad.  We ordered pretty simple dishes but they just had no flavor at all!  After trying it out I'm positive all the good reviews on here are employees or owners creating them.\n",
            "I've eaten here many times, but none as bad as last night.\n",
            "Service was excellent, and highly attentive.\n",
            "Food, absolutely horrible.\n",
            "\n",
            "My expectation was they would serve a steak on par with their seafood. After all, they were charging 39 bucks for a ribeye. \n",
            "What I was hoping for was a 1- 1-1/2' thick steak, cooked Pittsburgh style as I had ordered. \n",
            "What I got a a 3/4 in thick piece of meat that was mostly fat, gristle, and in no way resembled Pittsburgh Style. \n",
            "Salad, similar to something you could get at Chick Filet\n",
            "Veggies, blah.\n",
            "Bread basket, ample, but day old, and if not, it certainly wasn't fresh. \n",
            "\n",
            "In addition to bad food, we were crammed into a small room where we were nuts to butts with 6 other tables, listening to conversations ranging from someone's recent bout with pinkeye, and another couple who elected to speak entirely in French, until the waiter showed up, then it was like they turned off the French switch and suddenly began speaking English. \n",
            "\n",
            "I've had it with this place.\n",
            "If I'm going to pay 150 bucks for dinner, it'll be at Mortons, or Maestro where the steaks are 1-1/2 in thick, cooked to perfection, and half of it doesnt wind up on the plate as fat and gristle\n",
            "I have always been a fan of Burlington's deals, however I will not be shopping at this one again. I went to return a belt... pretty simple. Instead, I stood in the customer service line for 15 minutes thanks to an employee of Burlington buying/putting things on layaway. It took three other staff to help her out. There were no words said to me except \"Hold on\". I was pissed. When the lady was finally done, the employee at the service asked \"what do you want?\" Serious help is needed there!\n",
            " If you work in the area of \"customer service\" I think you should have some.\n",
            "Another night meeting friends here.  I have to laugh.  Waited another 20 minutes for my beer to be refilled at the bar.  A girl even took my empty without even asking if I wanted a refill.  A new brunette girl that I don't recognize left the bar and sat down with her guy friends on the customer side AT 9:25 ON A FRIDAY NIGHT.  Another bartender had to ask her to come back and work.  Management....  Pull your head out of your ass!  Sad to watch....  I need to talk my friends into another place!\n",
            "Not busy at all but took nearly 45 min to get our meal.  Ordered the trout and was shocked to see lots and lots of bones. Hmmmmm. Well asked the waitress about it and she said \"they try the best they can\"  hmmmmmm isn't this a \"fish\" restaurant? \n",
            "They comped the trout but still not sure I would go back.\n",
            "Yikes, reading other reviews I realize my bad experience wasn't unique. As a server I make a very laid back customer. I like pretty much everything I eat and don't require a lot of attention from the waiter. \n",
            "\n",
            "La Piccola Cucina would benefit from just one extra person in the front of the house. Our guy, though adorable and friendly, was too busy to refill our drinks and to remember to bring our appetizer (though charged us for it). The ahi tuna (highly recommended over the other fish options he said) was so overcooked it was the color and consistency of chicken. Like other reviewers mentioned, he was frantic and made that clear to every customer. At one point I even saw him in the kitchen cooking - they need another person! \n",
            "\n",
            "I left super stressed out from the experience, which is very, very unusual for me. You can either have bad service or bad food, but not both.\n",
            "This is my first year participating in Arizona to sell clothing, just wandering how busy its goona be in there ??? little worried, can any one reply me ??\n",
            "really, I can't believe this place has received such high reviews from people.\n",
            "\n",
            "my lady and i walked in, and were greeted rather rudely by a pretentious bitch at the front with a monotone \"name please?\", instead of a warm, friendly, french welcome to this rather charming looking place. we didn't have a reservation, which from the looks of how dead empty the place was, didn't seem like a problem. until the hostess whisked through her reservation list and explained she'd try to \"fit us in\". it was 6pm, the place had one other couple dining, and we were informed that the next party was arriving at 7:30. we weren't really looking to hang out. it would have be awfully lovely if yelpers would have informed of the need to MAKE RESERVATIONS before going in here, because if you don't, you're automatically a piece of shit according to the staff. sorry, i don't plan very much ahead. next time i will, and it won't be here.\n",
            "\n",
            "anyways, after debating for a minute or two with herself, the hostess decided we could \"squeeze in\" and sat us right on top of the other couple inside the restaurant. she informed us that we could cork our own wine, for their low low price of 9 fucking dollars. in words only office space could adequately describe,\"coup des tartes, what is it exactly that you do?\". \n",
            "\n",
            "the food was good, nothing spectacular for $25+ a plate. we got a salad with apples and nuts and bleu cheese, which was served warm. now this is the first warm salad i've had all of my life, and by no means am i some sort of wordly food expert, but i believe salad should be cold and crisp. not soggy and warm. yuck. the cordon bleu was tasty and filling, however, the wobbly table was an extreme annoyance when cutting through the chicken, but it was solved (with no thanks to the staff) by placing a jack-in-the-box gift card under the leg. i will admit, the mashed potatoes are some of the creamiest and best i've ever had. my lady got the pork tenderloin, which had a strange peppery-sweet pairing which i didn't particularly care for. we were broke for desert, so unfortunately we didn't get to try any tarts. you would think for a place that promotes itself as being so poise would get some fancier menus, other than a $.15 piece of cardstock copied in bulk at kinkos.\n",
            "\n",
            "this place was rather disappointing. i honestly thought it would be way more comfy and welcoming inside but it really wasn't. it presents itself as being so high class and french and oh-la-la wee wee, but when it comes down to it, it's still located in a tiny shack of a house at 16th street and highland. maybe if it were located in the biltmore i'd give it a bit more cred.\n",
            "I was really excited about this event, maybe my expectations were too high, because I was really underwhelmed.  Very little shade, ran out of water, by the time I got there at 1pm (other obligations) 20% of the vendors were out of food, and by the time I ate my way through some, everyone else was already out.   60 bucks for that?  I don't think so.  \n",
            "\n",
            "And when I mentioned it to \"the person in charge\", he was patronizing, invalidating and kept telling me how happy the chefs were.  Well, I'm your consumer, big guy.  \n",
            "\n",
            "Sorry, Devoured, wish it could have been better. \n",
            "\n",
            "What food I did try, was pretty good.\n",
            "The beach paradise of Ixtapa-Zihuatenejo awaited us....unfortunately, so did a connecting flight in ARIZONA of all places...\n",
            "\n",
            "Long story short....1/3 of our luggage did not arrive at our final destination, various articles of our clothing are still missing as well as my cell phone charger...not to mention security forcing us to open sealed containers of my son's baby food and milk for litmus tests...\n",
            "\n",
            "I looked on the bright side....at least we didn't get deported =)\n",
            "See the huge sign outside that says $2.50? Yeah, that's for shirts only. I was caught off guard when I brought in my skirt and got nailed for about $3.79. As I sadly handed over my $5 bill, the lady at the counter said a lot of people had told her they felt betrayed by the sign outside. No kidding. I had originally been heading down the street to Regal on 7th Ave.--who does a fine job in half the pickup time--for the \"advertised\" fake price here. Trying to make the best of it,  I told the lady at least I'd learned they had an onsite alteration service, and maybe that was worth a buck. UH OH. Picked up my skirt a few weeks later with some Jersey Shore-type chick at the counter, and was told I owed another $3.79. I flat-out refused to pay twice. She screamed at me, chased me to my car, wrote down my license plate and said the police would be visiting. Ni-i-i-ice.\n",
            "Unless you are a regular or look like your wallet is fat don't expect the best service.  Entrees cost about 20-38 bucks and Anti pasta runs between 5-20 desserts are all 8 bucks unless you get aged balsamic vinegar.  They also have a big cheese selection. \n",
            "\n",
            "I made reservations for this restaurant and they sat us by the back door.  The waiter never once explained anything on the menu or suggested a wine to go with our meal.  He didn't even tell us about the cheese menu except to check on the one we wanted.  The other waiter for a table near ours went out of his way to explain the menu and go as far as to tell his tables how to eat the cheese ( it comes with jelly's and such) .  \n",
            "\n",
            "My meal was good I had the duck.  She had the salmon which tasted very fishy.  Not fresh at all.  The wine was 40 bucks for a carafe ( maybe if we would have got a 100 dollar bottle we would have gotten better service)  The Dessert was good it was a rich chocolate cake with nuts and chocolate sauce. Nothing too special.  \n",
            "\n",
            "Overall the experience made the meal not worth the 115 bucks in these times not to mention they overcharged my girls card for the drinks at the bar.  If we dont want to give 25% don't charge it plain and simple.  \n",
            "\n",
            "Just a warning reservations and customer service mean nothing.  We mentioned our complaints to the waiter and manager.  We received an apology but how bout comp our dessert or something.  Hyatt Gainey staff is also rude for the most part.  \n",
            "\n",
            "Two thumbs way down.\n",
            "Take your money elsewhere, unless you've got kids.  I really try to like this place.  A family member signed me up for the discount card, so I've been going more often, but I just don't love it.  It's simply ok, but the prices are outrageous.  And the sounds and animatronics are a huge distraction from the so-so food.  The cocktails are alright, but, again, the price is not right.  The ony thing fun about the place for an adult is the gift shop and the light-up cocktail glasses (which cost extra.)  I've seen a lot of happy families in here though, so I bet it's better if you have little ones to bring along.\n",
            "My friend kept telling me how good their lunches are... I tried it, but it didn't do anything for me.  The sandwich tasted like something you can get at Safeway, but twice the price.  For being called a Purveyor of Fine Foods, I expected the food to taste like not just any ordinary sandwich. \n",
            "\n",
            "What I had:\n",
            "Ham, cheese with portobello mushrooms on a panini with 2 sides of pasta, one was parmesan and the other was macaroni. Maybe it was the panini bread. I think the parmesan pasta was made there, but I know for sure the macaroni was not made there, it tasted like something I had at a picnic which was in a carton. It wasn't anything to brag home about. \n",
            "\n",
            "I found out the head chef made my sandwich... keep him in the kitchen and away from the customers, no personality whatsoever. I asked for his suggestion, and his reaction was \"anythings good.\" \n",
            "\n",
            "Normally, when I go to AJ's, I go for their desserts. Their desserts are fabulous and you can get individual servings at a reasonable price.  I can take an assortment home and have them for later. \n",
            "\n",
            "Our company does use them for their catering and I've had many compliments on their dishes, but pass on the lunches.\n",
            "Absolutely horrendous.  This post office will lose your mail (repeatedly), laugh about it to your face, lie about it, blow it off, make it seem like it's your fault and make you wish you had sent it UPS.\n",
            "Other than the really great happy hour prices, its hit or miss with this place. More often a miss. :(\n",
            "\n",
            "The food is less than average, the drinks NOT strong ( at least they are inexpensive) , but the service is truly hit or miss.\n",
            "\n",
            "I'll pass.\n",
            "Been here for many years but have seen a recent decline in quality service. Felt scolded by the bartender today at lunch. Felt disrespected as a customer. Service sucks stay away. \"bad service = bad review\"\n",
            "This place is not there anymore.\n",
            "Rarely do I give a 1 star rating but after four attempts in four months to get my fax line and internet rolling I can honestly say they have earned this.  After finally showing up for a scheduled appointment the tech told us our internet was fine but we disagree because we are getting the same speed we had with Qwest in Cave Creek (SLOW)  and they are charging us $64 a month for what they call the upgraded 20mbs.  Furthermore, I tried to send out a couple faxes after he left and the line is not responding.  Im going to drive down to the store tomorrow and demand that they remove the fees for service I have not received and just get efax.  As I noted in the first review we were excited to upgrade to faster internet so that makes this experience even worse.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0GZ6NN4yEvos"
      },
      "source": [
        "We can start to see that there are certain quantitative differences between highly rated reviews and poorly rated reviews. Certain words, for example, 'delightful', 'impressive', 'amazing', might be more associated with 4 or 5 star reviews. However one might be able to see that these words might also be present in a 2 star review. For example: \"The seating and ambience were impressive, but the food served to us was not\". \n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J4PQg8FhEvow"
      },
      "source": [
        "**It is not really the presence of individual words that gives us an indication of the stars given to a review, but more  the *relative occurrence* of these words in each review that might give us an indication of a user's rating.**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TQHod14KEvoz"
      },
      "source": [
        "#### Word Clouds\n",
        "\n",
        "Another way to take a look at the most prominent words in any given star rating is through the use of word clouds. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oHY5IhnKEvo8"
      },
      "source": [
        "Edit the value in the cell below to see the word cloud for each star rating."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FtMnf1zLEvo_",
        "cellView": "form",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 240
        },
        "outputId": "d83e1332-7b5d-4132-ef84-682648469e9f"
      },
      "source": [
        "#@title Word cloud for differently rated reviews\n",
        "num_stars =  None#@param {type:\"integer\"}\n",
        "this_star_text = ''\n",
        "for t in yelp[yelp['stars'] == num_stars]['text'].values: # form field cell\n",
        "    this_star_text += t + ' '\n",
        "    \n",
        "wordcloud = WordCloud()    \n",
        "wordcloud.generate_from_text(this_star_text)\n",
        "plt.figure(figsize=(14,7))\n",
        "plt.imshow(wordcloud, interpolation='bilinear')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-2-cfb9f7641997>\u001b[0m in \u001b[0;36m<cell line: 4>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mnum_stars\u001b[0m \u001b[0;34m=\u001b[0m  \u001b[0;36m2\u001b[0m\u001b[0;31m#@param {type:\"integer\"}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mthis_star_text\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m''\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0;32mfor\u001b[0m \u001b[0mt\u001b[0m \u001b[0;32min\u001b[0m \u001b[0myelp\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0myelp\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'stars'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mnum_stars\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'text'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;31m# form field cell\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m     \u001b[0mthis_star_text\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mt\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m' '\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'yelp' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Nn4v3upxEvpL"
      },
      "source": [
        "**What are the differences between the reviews that have 1, 2, 3, 4, and 5 stars?**\n",
        "\n",
        "*As* we can see, in this case, the word cloud does not give us a great deal of distinguishing information between reviews that have 1, 2, 3, 4, or 5 stars. All these reviews seem to prominently feature words such as 'place', 'food', 'service' and 'table'. Human intuition will only get us so far. \n",
        "\n",
        "Before we go any further, we will need to clean up our text."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W8raBKzTEvpM"
      },
      "source": [
        "## Text Preprocessing"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pby7LlwhEvpN"
      },
      "source": [
        "#### Tokenization\n",
        "\n",
        "- Convert each review from a single string into a list of words (this is a process known as tokenizaton). \n",
        "- NLP algorithms require a list of words as arguments and not actual sentences. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7XOaa1uEEvpY",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "bcd0df7d-3383-48c3-bbb5-d3a5d5f8e610"
      },
      "source": [
        "#@title Basic tokenization example\n",
        "example_text = \"All the people I spoke to were super nice and very welcoming.\" #@param {type:\"string\"}\n",
        "tokens = word_tokenize(example_text)\n",
        "tokens"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['All',\n",
              " 'the',\n",
              " 'people',\n",
              " 'I',\n",
              " 'spoke',\n",
              " 'to',\n",
              " 'were',\n",
              " 'super',\n",
              " 'nice',\n",
              " 'and',\n",
              " 'very',\n",
              " 'welcoming',\n",
              " '.']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QKCP5Q_LEvpg"
      },
      "source": [
        "#### Stopwords\n",
        "\n",
        "We can see that certain particular words might be associated with 4 or 5 star reviews, and some words would be associated with 1 or 2 star reviews. However, at the same time, there are some words that do not really possess any relevant information for our current problem. In the field of NLP there is a concept of words that are \"stopwords\" - words that exist to provide grammatical structure, but do not convey information about the particular subject. Edit the cell below to see if a given word is a stop word."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Nqq4w-ZrEvpj",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "fee3bee2-07e3-4a3a-a2c2-2dbaa774c9cd"
      },
      "source": [
        "#@title Check if a word is a stop word\n",
        "example_word = \"the\" #@param {type:'string'}\n",
        "if example_word.lower() in STOP_WORDS:\n",
        "  print (example_word + \" is a stop word.\")\n",
        "else:\n",
        "  print (example_word + \" is NOT a stop word.\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "the is a stop word.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5vubl4ejEvpv"
      },
      "source": [
        "We would like to remove these stopwords from the user reviews."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EUZ35Ay9Evpy"
      },
      "source": [
        "Tokenization and removal of stop words are universal to nearly every NLP application. In some cases, additional cleaning may be required (for example, removal of proper nouns, removal of digits) but we can build a text preprocessing function with these \"base\" cleaning steps."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LPsaFxJiEvp9"
      },
      "source": [
        "Putting all these together, we can come up with a text cleaning function that we can apply to all of our reviews."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dcX6gwcFaFRd"
      },
      "source": [
        "Using Spacy\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ldEPkz8NcI6_",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        },
        "outputId": "68c9d8e0-a1ec-4466-e2b5-246536fc8c78"
      },
      "source": [
        "nlp = en_core_web_md.load()\n",
        "doc = nlp(u\"We are running out of time! Are we though?\")\n",
        "doc\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-3-a2e2c48ec29c>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mnlp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0men_core_web_md\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mdoc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnlp\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mu\"We are running out of time! Are we though?\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mdoc\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'en_core_web_md' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EWyXczXkePiI"
      },
      "source": [
        "The doc object has a lot of nice properties. For instance you can get the text of each of the words and the length of each of the words."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uQifXTc_DYK7"
      },
      "source": [
        "doc = nlp(u\"We are running out of time! Are we though?\")\n",
        "token = doc[0] # Get the first word in the text.\n",
        "assert token.text == u\"We\" # Check that the token text is 'We'.\n",
        "assert len(token) == 2 # Check that the length of the token is 2."
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vUI5yFeoekLQ"
      },
      "source": [
        "It has some word vectors that we can use. Though note that it doesn't have all the words. Let's import a new dataset of word (this may take a minute or so):"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m-yAoiZLLjbl"
      },
      "source": [
        "We can get the word embedding of a particular word in our document as follows:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "E0tBmojQeNhn",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5236bead-8110-4edd-e7e9-4854afe402b4"
      },
      "source": [
        "doc = nlp(u\"I like apples\")\n",
        "print(doc)\n",
        "appleVariable = doc[2]\n",
        "\n",
        "print(appleVariable.vector) # Each word is being represented by 300 dimensional vector embedding"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "I like apples\n",
            "[-0.6334     0.18981   -0.53544   -0.52658   -0.30001    0.30559\n",
            " -0.49303    0.14636    0.012273   0.96802    0.0040354  0.25234\n",
            " -0.29864   -0.014646  -0.24905   -0.67125   -0.053366   0.59426\n",
            " -0.068034   0.10315    0.66759    0.024617  -0.37548    0.52557\n",
            "  0.054449  -0.36748   -0.28013    0.090898  -0.025687  -0.5947\n",
            " -0.24269    0.28603    0.686      0.29737    0.30422    0.69032\n",
            "  0.042784   0.023701  -0.57165    0.70581   -0.20813   -0.03204\n",
            " -0.12494   -0.42933    0.31271    0.30352    0.09421   -0.15493\n",
            "  0.071356   0.15022   -0.41792    0.066394  -0.034546  -0.45772\n",
            "  0.57177   -0.82755   -0.27885    0.71801   -0.12425    0.18551\n",
            "  0.41342   -0.53997    0.55864   -0.015805  -0.1074    -0.29981\n",
            " -0.17271    0.27066    0.043996   0.60107   -0.353      0.6831\n",
            "  0.20703    0.12068    0.24852   -0.15605    0.25812    0.007004\n",
            " -0.10741   -0.097053   0.085628   0.096307   0.20857   -0.23338\n",
            " -0.077905  -0.030906   1.0494     0.55368   -0.10703    0.052234\n",
            "  0.43407   -0.13926    0.38115    0.021104  -0.40922    0.35972\n",
            " -0.28898    0.30618    0.060807  -0.023517   0.58193   -0.3098\n",
            "  0.21013   -0.15557   -0.56913   -1.1364     0.36598   -0.032666\n",
            "  1.1926     0.12825   -0.090486  -0.47965   -0.61164   -0.16484\n",
            " -0.41134    0.19925    0.059183  -0.20842    0.45223    0.27697\n",
            " -0.20745    0.025404  -0.28874    0.040478  -0.22275   -0.43323\n",
            "  0.76957   -0.054327  -0.35213   -0.30842   -0.48791   -0.35564\n",
            "  0.19813   -0.094767  -0.50918    0.18763   -0.087555   0.37709\n",
            " -0.1322    -0.096913  -1.9102     0.55813    0.27391   -0.077744\n",
            " -0.43933   -0.10367   -0.24408    0.41869    0.11659    0.27454\n",
            "  0.81021   -0.11006    0.43131    0.29095   -0.49548   -0.31958\n",
            " -0.072506   0.020286   0.2179     0.22032   -0.29212    0.75639\n",
            "  0.13598    0.019736  -0.83104    0.22836   -0.28669   -1.0529\n",
            "  0.052771   0.41266    0.50149    0.5323     0.51573   -0.31806\n",
            " -0.4619     0.21739   -0.43584   -0.41382    0.042237  -0.57179\n",
            "  0.067623  -0.27854    0.090044   0.20633    0.024678  -0.57703\n",
            " -0.020183  -0.53147   -0.37548   -0.12795   -0.093662  -0.0061183\n",
            "  0.20221   -0.62296   -0.29746    0.26935    0.59009   -0.50382\n",
            " -0.69757    0.20157   -0.33592   -0.45766    0.14061    0.22982\n",
            "  0.044046   0.26386    0.02942    0.34095    1.1496    -0.15555\n",
            " -0.064071   0.30139    0.024211  -0.63515   -0.73347   -0.10346\n",
            " -0.22637   -0.056392  -0.16735   -0.097331  -0.19206   -0.18866\n",
            "  0.15116   -0.038048   0.70205    0.11586   -0.14813    0.0095166\n",
            " -0.33804   -0.10158   -0.23829   -0.22759    0.092504  -0.29839\n",
            " -0.39721    0.26092    0.34594   -0.47396   -0.25725   -0.19257\n",
            " -0.53071    0.1692    -0.47252   -0.17333   -0.40505    0.046446\n",
            " -0.04473    0.33555   -0.5693     0.31591   -0.21167   -0.31298\n",
            " -0.45923   -0.083091   0.086822   0.01264    0.43779    0.12651\n",
            "  0.30156    0.022061   0.26549   -0.29455   -0.14838    0.033692\n",
            " -0.37346   -0.075343  -0.56498   -0.24207   -0.69351   -0.20277\n",
            " -0.0081185  0.030971   0.53615   -0.16613   -0.84087    0.74661\n",
            "  0.029132   0.46936   -0.49755    0.40954   -0.022558   0.21497\n",
            " -0.049528  -0.039799   0.46165    0.26456    0.32985   -0.04219\n",
            " -0.099599  -0.17312   -0.476     -0.019048  -0.41888   -0.2685\n",
            " -0.65281    0.068773  -0.23881   -1.1784     0.25504    0.61171  ]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NNPzJouoJj3f"
      },
      "source": [
        "The word 'Apple' is represented by 300 dimensional vector embedding\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lmX2ENExhwj2"
      },
      "source": [
        "### Get similarity of Two Words"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LV67qViFjari",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "241c19f0-085f-4f32-bbf4-317c45f1c355"
      },
      "source": [
        "#@title  { display-mode: \"code\" }\n",
        "similar_words_doc = nlp(u\"apples oranges\")\n",
        "w1 = similar_words_doc[0]\n",
        "w2 = similar_words_doc[1]\n",
        "print(w1.similarity(w2))\n",
        "\n",
        "dissimilar_words_doc = nlp(u\"doorknob phone\")\n",
        "w3 = dissimilar_words_doc[0]\n",
        "w4 = dissimilar_words_doc[1]\n",
        "print(w3.similarity(w4))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0.77809423\n",
            "0.14618301\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0bwasn11BfD4"
      },
      "source": [
        "As we saw before, the language in 4 star reviews is quite similar to the language in 5 star reviews. So the text in those reviews might not be very useful and we can drop those rows from our data.\n",
        "\n",
        "Although the text in the 3 star reviews is not very similar to the 1 or 2 star reviews, it is quite different from the language used in the 5 star reviews. So we could actually group those reviews together with the 1 and 2 star reviews.\n",
        "\n",
        "In order to reduce our problem to a **binary classification** problem, we will:\n",
        "\n",
        " - remove all 4 star reviews\n",
        " - label 5 star reviews as 'good'\n",
        " - label 1, 2, 3 star reviews as 'bad'"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xn__oGIOEvrA"
      },
      "source": [
        "Get rid of 4 star reviews."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TrgQp7gEEvrC"
      },
      "source": [
        "yelp = yelp[yelp.stars != 4]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bNgLAFNpEvrG"
      },
      "source": [
        "### Re-categorize reviews\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "x2uO1pbAEvrI",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 203
        },
        "outputId": "d138da73-c4b9-47dd-9b85-fa50ebf2e783"
      },
      "source": [
        "#@title  { display-mode: \"code\" }\n",
        "def is_good_review(stars):\n",
        "    if stars > 3:             \n",
        "        return True\n",
        "    else:\n",
        "        return False\n",
        "    \n",
        "yelp['is_good_review'] = yelp['stars'].apply(is_good_review)\n",
        "yelp.head()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>stars</th>\n",
              "      <th>text</th>\n",
              "      <th>cool</th>\n",
              "      <th>useful</th>\n",
              "      <th>funny</th>\n",
              "      <th>is_good_review</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>5</td>\n",
              "      <td>My wife took me here on my birthday for breakf...</td>\n",
              "      <td>2</td>\n",
              "      <td>5</td>\n",
              "      <td>0</td>\n",
              "      <td>True</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>5</td>\n",
              "      <td>I have no idea why some people give bad review...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>True</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>5</td>\n",
              "      <td>Rosie, Dakota, and I LOVE Chaparral Dog Park!!...</td>\n",
              "      <td>1</td>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "      <td>True</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>5</td>\n",
              "      <td>General Manager Scott Petello is a good egg!!!...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>True</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>5</td>\n",
              "      <td>Drop what you're doing and drive here. After I...</td>\n",
              "      <td>7</td>\n",
              "      <td>7</td>\n",
              "      <td>4</td>\n",
              "      <td>True</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   stars  ... is_good_review\n",
              "0      5  ...           True\n",
              "1      5  ...           True\n",
              "2      5  ...           True\n",
              "3      5  ...           True\n",
              "4      5  ...           True\n",
              "\n",
              "[5 rows x 6 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XfIUrRtEWr3H"
      },
      "source": [
        "## One-Hot Vectors\n",
        "\n",
        "How do we convert our text to numbers in a structured way that we can feed into a machine learning algorithm? One way to do it is to use a concept called \"one-hot encoding\". We can see this concept with the following example. Suppose we have a sentence \"great tacos at this restaurant\". Its one-hot encoding would be."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ucOjGk7qXjbb",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a819a526-ae76-4434-e85b-eb737cd03288"
      },
      "source": [
        "#@title Example: one-hot encoding of 'great tacos at this restaurant' { vertical-output: true, display-mode: \"both\" }\n",
        "print('{:^5}|{:^5}|{:^4}|{:^4}|{:^10}'.format('great', 'tacos', 'at','this','restaurant'))\n",
        "print('--------------------------------------------')\n",
        "print('{:^5}|{:^5}|{:^4}|{:^4}|{:^10}'.format('1', '0', '0','0','0'))\n",
        "print('{:^5}|{:^5}|{:^4}|{:^4}|{:^10}'.format('0', '1', '0','0','0'))\n",
        "print('{:^5}|{:^5}|{:^4}|{:^4}|{:^10}'.format('0', '0', '1','0','0'))\n",
        "print('{:^5}|{:^5}|{:^4}|{:^4}|{:^10}'.format('0', '0', '0','1','0'))\n",
        "print('{:^5}|{:^5}|{:^4}|{:^4}|{:^10}'.format('0', '0', '0','0','1'))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "great|tacos| at |this|restaurant\n",
            "--------------------------------------------\n",
            "  1  |  0  | 0  | 0  |    0     \n",
            "  0  |  1  | 0  | 0  |    0     \n",
            "  0  |  0  | 1  | 0  |    0     \n",
            "  0  |  0  | 0  | 1  |    0     \n",
            "  0  |  0  | 0  | 0  |    1     \n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EpZ11S8lEvrM"
      },
      "source": [
        "## Bag of Words"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "43hbR0vha7E3"
      },
      "source": [
        "- One-hot encoding is a way to represent individual words as vectors\n",
        "- Bag of words as a way to represent sentences (or larger pieces of text) as the **sum** of the one-hot encoding vectors of each of the words. \n",
        "\n",
        "Example:\n",
        "**\"The food was great. The ambience was also great.\"**\n",
        "\n",
        "1. Define our vocabulary - This is *each unique word* in the review. \n",
        "  So our vocabulary is **[the, food, was, great, ambience, also]**.\n",
        "\n",
        "2. Determine one hot encoding \n",
        "\n",
        "> \n",
        "      the = (1,0,0,0,0,0)\n",
        "      food = (0,1,0,0,0,0)\n",
        "      was = (0,0,1,0,0,0)\n",
        "      great = (0,0,0,1,0,0)\n",
        "      ambience = (0,0,0,0,1,0)\n",
        "      also = (0,0,0,0,0,1).\n",
        "\n",
        "3. Represent as a bag of words.\n",
        " - Bag of words vector will also only be 6 elements long\n",
        " - To construct it, we can start off with a (0,0,0,0,0,0) vector, and then pass through each word in the review. For each word we encounter, we simply add its one hot encoding to our vector! So for our review, the bag of words representation will be\n",
        "\n",
        "      **(2,1,2,2,1,1)**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NbFU78B5bXka"
      },
      "source": [
        "## Creating our Bag of Words\n",
        "\n",
        "We want to select the features for our model and the output classes from our data. What are the features? We are only using the review text to make predictions for our model. And the output classes are the 'good' and 'bad' review classes we created just above. \n",
        "\n",
        "By convention, we represent our entire set of features as X, and our target output as y. Running the cell below will create the relevant X and y for our problem."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6t6HQm1vEvrQ"
      },
      "source": [
        "X = yelp['text']\n",
        "y = yelp['is_good_review']"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xhRyg_YEeA5t"
      },
      "source": [
        "Running the cell below will create an object we can use to *transform* each piece of raw text into a bag of words vector.\n",
        "CountVectorizer is a useful class we can call from scikit-learn that will help us create this object. It even has a helpful parameter that we can set to our tokenize function to preprocess the raw text."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yrSQAeKjAiXJ"
      },
      "source": [
        "#@title Initialize the text cleaning function { display-mode: \"form\" }\n",
        "def tokenize(text):\n",
        "    clean_tokens = []\n",
        "    for token in nlp(text):\n",
        "        if (not token.is_stop) & (token.lemma_ != '-PRON-') & (not token.is_punct): # -PRON- is a special all inclusive \"lemma\" spaCy uses for any pronoun, we want to exclude these \n",
        "            clean_tokens.append(token.lemma_)\n",
        "    return clean_tokens"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "blZ7RJ2zEvrU"
      },
      "source": [
        "bow_transformer = CountVectorizer(analyzer=tokenize, max_features=1600).fit(X)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RaheGj_RmKW7"
      },
      "source": [
        "See entire vocabulart. Index = position of each word in the vocabulary."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6TjdgYVxmKgd"
      },
      "source": [
        "bow_transformer.vocabulary_"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nGEf8h1lEvrY"
      },
      "source": [
        "We can see the length of the vocabulary stored in the transformer object by running the cell below. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "upN1gxm5Evrb"
      },
      "source": [
        "len(bow_transformer.vocabulary_)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YsFa7Nu5Evr4"
      },
      "source": [
        "Transformer to transform our entire training set (X) into a series of bag of words vectors:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VRJJe2HGEvr6"
      },
      "source": [
        "X = bow_transformer.transform(X)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P8AUxyLHEvr_"
      },
      "source": [
        "## Training a Baseline Classification Model (Logistic Regression)\n",
        "\n",
        "Our classification problem is a classic two-class classification problem, and so we will use the tried and tested **Logistic Regression** machine learning model."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gIpbdNZwgRTn"
      },
      "source": [
        "# import the logistic regression model from scikit-learn\n",
        "logistic_model = LogisticRegression()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q459D2jwaUCI"
      },
      "source": [
        "We will use 20% of our data as test data. If you run the cell below, it will randomly split the data such that 80% of it is training data and 20% of it is data we can use to test the predictions from our trained model."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PThy6pNUEvsA"
      },
      "source": [
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=101)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u8K4C0iQEvsE"
      },
      "source": [
        "### Train Model\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sNn21MqmEvsG",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 327
        },
        "outputId": "8d9d3ffd-389c-4198-e085-d26a39f524b3"
      },
      "source": [
        "#@title  { vertical-output: true, display-mode: \"code\" }\n",
        "logistic_model.fit(X_train, y_train)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-29-d9c40aaea4d9>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m#@title Solution hidden { display-mode: \"form\" }\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mlogistic_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[1;32m   1525\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1526\u001b[0m         X, y = check_X_y(X, y, accept_sparse='csr', dtype=_dtype, order=\"C\",\n\u001b[0;32m-> 1527\u001b[0;31m                          accept_large_sparse=solver != 'liblinear')\n\u001b[0m\u001b[1;32m   1528\u001b[0m         \u001b[0mcheck_classification_targets\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1529\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclasses_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munique\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/sklearn/utils/validation.py\u001b[0m in \u001b[0;36mcheck_X_y\u001b[0;34m(X, y, accept_sparse, accept_large_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, multi_output, ensure_min_samples, ensure_min_features, y_numeric, warn_on_dtype, estimator)\u001b[0m\n\u001b[1;32m    753\u001b[0m                     \u001b[0mensure_min_features\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mensure_min_features\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    754\u001b[0m                     \u001b[0mwarn_on_dtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mwarn_on_dtype\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 755\u001b[0;31m                     estimator=estimator)\n\u001b[0m\u001b[1;32m    756\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mmulti_output\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    757\u001b[0m         y = check_array(y, 'csr', force_all_finite=True, ensure_2d=False,\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/sklearn/utils/validation.py\u001b[0m in \u001b[0;36mcheck_array\u001b[0;34m(array, accept_sparse, accept_large_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, ensure_min_samples, ensure_min_features, warn_on_dtype, estimator)\u001b[0m\n\u001b[1;32m    529\u001b[0m                     \u001b[0marray\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0marray\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcasting\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"unsafe\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    530\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 531\u001b[0;31m                     \u001b[0marray\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0morder\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0morder\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    532\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mComplexWarning\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    533\u001b[0m                 raise ValueError(\"Complex data not supported\\n\"\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/numpy/core/_asarray.py\u001b[0m in \u001b[0;36masarray\u001b[0;34m(a, dtype, order)\u001b[0m\n\u001b[1;32m     81\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     82\u001b[0m     \"\"\"\n\u001b[0;32m---> 83\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0morder\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0morder\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     84\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     85\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/pandas/core/series.py\u001b[0m in \u001b[0;36m__array__\u001b[0;34m(self, dtype)\u001b[0m\n\u001b[1;32m    795\u001b[0m               dtype='datetime64[ns]')\n\u001b[1;32m    796\u001b[0m         \"\"\"\n\u001b[0;32m--> 797\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    798\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    799\u001b[0m     \u001b[0;31m# ----------------------------------------------------------------------\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/numpy/core/_asarray.py\u001b[0m in \u001b[0;36masarray\u001b[0;34m(a, dtype, order)\u001b[0m\n\u001b[1;32m     81\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     82\u001b[0m     \"\"\"\n\u001b[0;32m---> 83\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0morder\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0morder\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     84\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     85\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/pandas/core/arrays/numpy_.py\u001b[0m in \u001b[0;36m__array__\u001b[0;34m(self, dtype)\u001b[0m\n\u001b[1;32m    209\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    210\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__array__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndarray\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 211\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_ndarray\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    212\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    213\u001b[0m     \u001b[0m_HANDLED_TYPES\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndarray\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnumbers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mNumber\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/numpy/core/_asarray.py\u001b[0m in \u001b[0;36masarray\u001b[0;34m(a, dtype, order)\u001b[0m\n\u001b[1;32m     81\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     82\u001b[0m     \"\"\"\n\u001b[0;32m---> 83\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0morder\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0morder\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     84\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     85\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: could not convert string to float: \"just say no.\\n\\nthis place is truly awful. \\ntoo bad it's taking up valuable space on monroe.\""
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ApCzou0tEvsL"
      },
      "source": [
        "## Get Predictions"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Get our predictions.\n",
        "preds = logistic_model.predict(X_test)\n",
        "\n",
        "# Get the confusion matrix.\n",
        "cm = confusion_matrix(y_test, preds)\n",
        "\n",
        "# Get TP, FP, TN, and FN rates.\n",
        "TP = cm[0][0]\n",
        "TN = cm[1][1]\n",
        "FP = cm[0][1]\n",
        "FN = cm[1][0]\n",
        "\n",
        "# Calculate and print accuracy.\n",
        "accuracy = (TP + TN)/(TP + TN + FP + FN)\n",
        "print (\"The accuracy of the model is \" + str(accuracy*100) + \"%\")"
      ],
      "metadata": {
        "id": "P9KURPwet0ZL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eKgDnIcaEvsX"
      },
      "source": [
        "Not perfect, but definitely better than we would have expected at random (50%).\n",
        "\n",
        "Enter an example review to see if our model predicts it as a positive one or a negative one."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "euuR1VWWEvsX",
        "cellView": "form"
      },
      "source": [
        "#@title Enter an example review, and see if it is classified as good or bad\n",
        "example_review = \"good!!!!!!!!!!!!!\" #@param {type:'string'}\n",
        "prediction = logistic_model.predict(bow_transformer.transform([example_review]))\n",
        "\n",
        "if prediction:\n",
        "  print (\"This was a GOOD review!\")\n",
        "else:\n",
        "  print (\"This was a BAD review!\")\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}